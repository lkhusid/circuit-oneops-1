# This file is generated by chef
# Any manual changes will be overwritten!
# To make changes, edit the template in the kafka cookbook
#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
# 
#    http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# see kafka.server.KafkaConfig for additional details and defaults

############################# Socket Server Settings #############################

# The port the socket server listens on
port=<%= node['kafka']['port'] %>

# Enable auto creating of topics or not
auto.create.topics.enable=<%= node['kafka']['auto_create_topics_enable'] %>

# Enable delete topic or not
delete.topic.enable=<%= node['kafka']['delete_topic_enable'] %>

# Number of threads used to replicate messages from leaders. Increasing this value can
# increase the degree of I/O parallelism in the follower broker.
num.replica.fetchers=<%= node['kafka']['num_replica_fetchers'] %>

# Hostname the broker will bind to and advertise to producers and consumers.
# If not set, the server will bind to all interfaces and advertise the value returned from
# from java.net.InetAddress.getCanonicalHostName().
#host.name=localhost

# The number of threads handling network requests
num.network.threads=<%= node['kafka']['num_network_threads'] %>

# The number of threads doing disk I/O
num.io.threads=<%= node['kafka']['num_io_threads'] %>

# Number of requests to be queued - backlog for I/O threads
queued.max.requests = <%= node['kafka']['queued_max_requests'] %>

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=<%= node['kafka']['socket_send_buffer_bytes'] %>

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=<%= node['kafka']['socket_receive_buffer_bytes'] %>

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=<%= node['kafka']['socket_request_max_bytes'] %>

######################## Maximum message size ###############################

replica.fetch.max.bytes=<%= node['kafka']['replica_fetch_max_bytes'] %>
message.max.bytes=<%= node['kafka']['message_max_bytes'] %>

############################# Log Basics #############################

# The directory under which to store log files
log.dir=<%= node['kafka']['data_dir'] %>

# The number of logical partitions per topic per server. More partitions allow greater parallelism
# for consumption, but also mean more files.
num.partitions=<%= node['kafka']['num_partitions'] %>

############################# Log Flush Policy #############################

# The following configurations control the flush of data to disk. This is the most
# important performance knob in kafka.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data is at greater risk of loss in the event of a crash.
#    2. Latency: Data is not made available to consumers until it is flushed (which adds latency).
#    3. Throughput: The flush is generally the most expensive operation.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion
log.retention.hours=<%= node['kafka']['log_retention_hours'] %>

# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining
# segments don't drop below log.retention.bytes.
log.retention.bytes=<%= node['kafka']['log_retention_bytes'] %>

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=<%= node['kafka']['log_segment_bytes'] %>

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=<%= node['kafka']['log_cleanup_interval_ms'] %>

# Indicates whether to enable replicas not in the ISR set to be elected as leader as a last resort, even though doing so may result in data loss.
unclean.leader.election.enable=<%= node['kafka']['unclean_leader_election_enable'] %>

# The default replication factor for automatically created topics.
default.replication.factor=<%= node['kafka']['default_replication_factor'] %>


############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
<% zk_peers = Array.new %>
<% zk_client_port = node['kafka']['zk_clientPort'] %>
<% @zookeeper_cluster_peers.sort.each do |zk_peer| %>
<% zk_peers.push(zk_peer) %>
<% end -%>
zookeeper.connect=<%= (zk_peers.to_a).join(":#{zk_client_port},") %>:<%= zk_client_port %>

# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=<%= node['kafka']['zookeeper_connection_timeout_ms'] %>

zookeeper.session.timeout.ms = <%= node['kafka']['zookeeper_session_timeout_ms'] %>
zookeeper.sync.time.ms = <%= node['kafka']['zookeeper_sync_time_ms'] %>

# metrics reporter properties
kafka.metrics.polling.interval.secs=5
kafka.metrics.reporters=kafka.metrics.KafkaCSVMetricsReporter
kafka.csv.metrics.dir=<%= node['kafka']['kafka_csv_metrics_dir'] %>

# Disable csv reporting by default.
kafka.csv.metrics.reporter.enabled=false

########################## Replica ############################
# If a replica falls more than this many messages behind the leader, the leader will
# remove the follower from ISR and treat it as dead. The default value is 4000.
# Increase it to raise the ISR tolerance.
replica.lag.max.messages = <%= node['kafka']['replica_lag_max_messages'] %>
replica.lag.time.max.ms = <%= node['kafka']['replica_lag_time_max_ms'] %>

min.insync.replicas = <%= node['kafka']['min_insync_replicas'] %>

replica.fetch.max.bytes = <%= node['kafka']['replica_fetch_max_bytes'] %>
replica.fetch.wait.max.ms = <%= node['kafka']['replica_fetch_wait_max_ms'] %>
replica.high.watermark.checkpoint.interval.ms = <%= node['kafka']['replica_high_watermark_checkpoint_interval_ms'] %>
replica.socket.timeout.ms = <%= node['kafka']['replica_socket_timeout_ms'] %>
replica.socket.receive.buffer.bytes = <%= node['kafka']['replica_socket_receive_buffer_bytes'] %>

########################## Kafka offset management ##########################
offsets.topic.num.partitions = <%= node['kafka']['offsets_topic_num_partitions'] %>
offsets.topic.retention.minutes = <%= node['kafka']['offsets_topic_retention_minutes'] %>
offsets.retention.check.interval.ms = <%= node['kafka']['offsets_retention_check_interval_ms'] %>
offsets.topic.replication.factor = <%= node['kafka']['offsets_topic_replication_factor'] %>
offsets.commit.timeout.ms = <%= node['kafka']['offsets_commit_timeout_ms'] %>

########################## Producer Consumer ##########################
fetch.purgatory.purge.interval.requests = <%= node['kafka']['fetch_purgatory_purge_interval_requests'] %>
producer.purgatory.purge_interval.requests = <%= node['kafka']['producer_purgatory_purge_interval_requests'] %>

socket.timeout.ms = <%= node['kafka']['consumer_socket_timeout_ms'] %>
auto.commit.enable = <%= node['kafka']['consumer_auto_commit_enable'] %>
auto.commit.interval.ms = <%= node['kafka']['consumer_auto_commit_interval_ms'] %>
auto.offset.reset = <%= node['kafka']['consumer_auto_offset_reset'] %>
consumer.timeout.ms = <%= node['kafka']['consumer_timeout_ms'] %>

producer.type = <%= node['kafka']['producer_type'] %>
request.required.acks = <%= node['kafka']['producer_request_required_acks'] %>
compression.codec = <%= node['kafka']['producer_compression_codec'] %>
serializer.class = <%= node['kafka']['producer_serializer_class'] %>
compressed.topics = <%= node['kafka']['producer_compressed_topics'] %>


########################## Controlled shutdown ##########################
controlled.shutdown.enable = <%= node['kafka']['controlled_shutdown_enable'] %>
controlled.shutdown.max.retries = <%= node['kafka']['controlled_shutdown_max_retries'] %>
controlled.shutdown.retry.backoff.ms = <%= node['kafka']['controlled_shutdown_retry_backoff_ms'] %>

########################## Controller ##########################
controller.socket.timeout.ms = <%= node['kafka']['controller_socket_timeout_ms'] %>
controller.message.queue.size = <%= node['kafka']['controller_message_queue_size'] %>

################# Just placeholder to trigger Rolling Restart (value does not matter) ##############
rolling.restart = <%= node['kafka']['rolling_restart_trigger'] %>


